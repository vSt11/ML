{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5 : Vers la regression linéaire multiple et optimisation\n",
    "\n",
    "On considère que l'on connait les notes sur une année de n élèves dans p matières, ainsi que leurs notes à un concours en fin d'annee. L'année suivante, on  se demande si on ne pourrait pas prédire la note des étudiants au concours en fonction de leurs notes annuelle afin d'estimer leurs chances de réussite au concours.\n",
    "\n",
    "\n",
    "On va resoudre le problème à l'aide de la régression linéaire en dimension p>1 sans utiliser scikit-learn. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">QUESTION 5.1 :</span> \n",
    "\n",
    "A l'aide de la fonction 'SimulateObservations', simulez un jeu de donnees d'apprentissage [X_l,y_l] avec 30 observations et un jeu de test [X_t,y_t] avec 10 observations. Les observations seront en dimension p=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateObservations(n_train,n_test,p):\n",
    "    \"\"\"\n",
    "    n_train: number of training obserations to simulate\n",
    "    n_test: number of test obserations to simulate\n",
    "    p: dimension of the observations to simulate\n",
    "    \"\"\"\n",
    "\n",
    "    ObsX_train=20.*np.random.rand(n_train,p)\n",
    "    ObsX_tst=20.*np.random.rand(n_test,p)\n",
    "\n",
    "    RefTheta=np.random.rand(p)**3\n",
    "    RefTheta=RefTheta/RefTheta.sum()\n",
    "    print(\"The thetas with which the values were simulated is: \"+str(RefTheta))\n",
    "\n",
    "    ObsY_train=np.dot(ObsX_train,RefTheta.reshape(p,1))+1.5*np.random.randn(n_train,1)\n",
    "    ObsY_tst=np.dot(ObsX_tst,RefTheta.reshape(p,1))+1.5*np.random.randn(n_test,1)\n",
    "\n",
    "    return [ObsX_train,ObsY_train,ObsX_tst,ObsY_tst,RefTheta]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 5.1 :</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 5.2 :</span> \n",
    "\n",
    "On considere un modele linéaire en dimension p>1 pour mettre en lien les x[i,:] et les y[i], c'est a dire que np.dot(x[i,:],theta_optimal) doit etre le plus proche possible de y[i] sur l'ensemble des observations i. Dans le modèle linéaire multiple, theta_optimal est un vecteur de taille [p,1] qui pondère les différentes variables observées (ici les moyennes dans une matière). Coder alors une fonction qui calcule la moyenne des différences au carré entre ces valeurs en fonction de theta, *i.e.* la mean squared error (MSE) du modèle.\n",
    "\n",
    "### <span style=\"color:blue\">REPONSE 5.2 :</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CptMSE(theta_test, X, y_true):\n",
    "    y_pred = np.dot(X, theta_test)[:, np.newaxis]\n",
    "    MSE = np.mean((y_true - y_pred)**2)\n",
    "    return MSE\n",
    "\n",
    "theta_test=np.random.rand(p)\n",
    "theta_test=theta_test/theta_test.sum()\n",
    "\n",
    "MSE_test=CptMSE(theta_test, ObsX_train, ObsY_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">QUESTION 5.3 :</span> \n",
    "\n",
    "On va maintenant chercher le theta_test qui minimise cette fonction (il correspondra à theta_optimal), et ainsi résoudre le probleme d'apprentissage de regression lineaire multiple. Utiliser pour cela la fonction minimize de scipy.optimize\n",
    "\n",
    "\n",
    "De manière importante, la recherche des paramètres *theta_optimal* sera effectuée en utilisant les observations d'apprentissage (*ObsX_train* et *ObsY_train* en sortie *SimulateObservations*). La MSE obtenue sur les observations d'apprentissage avec *theta_optimal* sera comparée à celle obtenue avec les observations de test (*ObsX_tst* et *ObsY_tst* en sortie *SimulateObservations*) avec le même *theta_optimal*. Que constatez vous ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">REPONSE 5.3 :</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi optimiser le modèle par une descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CptMSE(X,y_true,theta_test):\n",
    "    y_pred=np.dot(X,theta_test)[:, np.newaxis]\n",
    "    #print(y_pred.shape)\n",
    "    #print(y_true.shape)\n",
    "    MSE=np.mean(np.power(y_pred-y_true,2.))\n",
    "\n",
    "    return MSE\n",
    "\n",
    "\n",
    "def gradientApprox(fct_to_minimize,theta_loc,X_loc,Y_loc,epsilon=1e-5):\n",
    "\n",
    "    fx=fct_to_minimize(X_loc,Y_loc,theta_loc)\n",
    "    #print(fx)\n",
    "    ApproxGrad=np.zeros(np.size(theta_loc))\n",
    "    veps=np.zeros(np.size(theta_loc))\n",
    "\n",
    "    for i in range(np.size(theta_loc)):\n",
    "        veps[:]=0.\n",
    "        veps[i]+=epsilon\n",
    "        ApproxGrad[i]=(fct_to_minimize(X_loc,Y_loc,theta_loc+veps)-fx)/epsilon\n",
    "    return ApproxGrad\n",
    "\n",
    "\n",
    "def analyticGradient(_, theta,x,y,epsilon=1e-5):\n",
    "    shape = theta.shape\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    grad_theta = (-2*np.dot((y-np.dot(x, theta)).T,x))/len(y)\n",
    "    grad_theta = grad_theta.reshape(shape)\n",
    "    return grad_theta\n",
    "\n",
    "def GradientDescent(fct_to_minimize,theta_init,X_loc,Y_loc,alpha=0.01,N=100, approx=True):\n",
    "    \"\"\"\n",
    "    Remark: the multiplicatory coefficient of the gradients will be \"alpha\" divided by the norm of the first gradient \n",
    "    \"\"\"\n",
    "\n",
    "    #init\n",
    "    l_thetas=[theta_init]\n",
    "    theta_curr=theta_init.copy()\n",
    "\n",
    "    #run the gradient descent\n",
    "    n=0\n",
    "    while n<N:\n",
    "        #approximate the gradient of fct_to_minimize w.r.t. theta_curr\n",
    "        if approx:\n",
    "            g=gradientApprox(fct_to_minimize,theta_curr,X_loc,Y_loc)\n",
    "        else:\n",
    "            g=analyticGradient(fct_to_minimize,theta_curr,X_loc,Y_loc)\n",
    "\n",
    "        #set the multiplicatory coefficient of the gradients\n",
    "        if n==0:\n",
    "            NormFirstGrads=np.linalg.norm(g)\n",
    "            coefMult=alpha/NormFirstGrads\n",
    "\n",
    "        #update theta\n",
    "        theta_curr=theta_curr-coefMult*g\n",
    "\n",
    "        #save the current state and increment n\n",
    "        l_thetas.append(theta_curr)\n",
    "        n+=1\n",
    "\n",
    "    return l_thetas\n",
    "\n",
    "theta_init = np.abs(np.random.randn(p))/10.\n",
    "thetas = GradientDescent(CptMSE, theta_init, ObsX_train, ObsY_train, approx=True)\n",
    "theta_optim = thetas[-1]\n",
    "\n",
    "mse_train = CptMSE(ObsX_train, ObsY_train, theta_optim)\n",
    "mse_test = CptMSE(ObsX_test, ObsY_test, theta_optim)\n",
    "\n",
    "mse_train_ref = CptMSE(ObsX_train, ObsY_train, RefTheta)\n",
    "mse_test_ref = CptMSE(ObsX_test, ObsY_test, RefTheta)\n",
    "\n",
    "print(\"Theta optimal --- \")\n",
    "print(\"Base d'apprentissage - MSE : \", mse_train)\n",
    "print(\"Base de test - MSE : \", mse_test)\n",
    "print(\"\")\n",
    "print(\"Theta de référence --- \")\n",
    "print(\"Base d'apprentissage - MSE : \", mse_train_ref)\n",
    "print(\"Base de test - MSE : \", mse_test_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
